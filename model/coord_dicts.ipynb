{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.abspath(''), '..', 'shared_libs'))\n",
    "import data_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data_target_cleared.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_transform.clear_data_base_line(\n",
    "    df, \n",
    "    '../shared_libs/data/default_values.pkl',\n",
    "    can_drop_rows=True, \n",
    "    force_rebuild_cached_data=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кэш-словарь с геоданными о городах США\n",
    "Данные получены через сервис OpenStreetMap\n",
    "\n",
    "Ключем словаря служит пара \\<state\\>, \\<city\\> (название города приводится к нижнему регистру)\n",
    "\n",
    "По каждому городу сохраняется следующая информация:\n",
    "* *type* - тип населенного пункта\n",
    "* *importance* - важность (по мнению OSM)\n",
    "* *boundingbox* - габаритный бокс населенного пункта\n",
    "* *lat* - широта центра города\n",
    "* *lng* - долгота центра города"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "geolocator = Nominatim(user_agent='myapplication')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['city'] = df['city'].apply(lambda x: str.lower(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_list = df.groupby(['state', 'city'])['target'].mean().index\n",
    "\n",
    "for city_item in cities_list:\n",
    "    key = data_transform.get_city_dict_key(city_item[0], city_item[1])\n",
    "    if key not in cities_dict:\n",
    "        try:\n",
    "            location = geolocator.geocode(key)\n",
    "            cities_dict[key] = {\n",
    "                'type': location.raw['type'],\n",
    "                'importance': location.raw['importance'],\n",
    "                'boundingbox': location.raw['boundingbox'],\n",
    "                'lat': location.raw['lat'],\n",
    "                'lng': location.raw['lon']\n",
    "            }\n",
    "        except Exception as ex:\n",
    "            print(city_item[0] + ', {' + city_item[1] + '}', ex)\n",
    "            \n",
    "with open('../shared_libs/data/cities_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(cities_dict, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для всех городов, которые не были найдены OSM по почтовому индексу были уточнены названия населенного пункта.\n",
    "\n",
    "Ниже приведены списки замен для ошибочных названий штатов и городов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_replace = [\n",
    "    ['Fl', 'FL'],\n",
    "    ['BA', 'FL']\n",
    "]\n",
    "\n",
    "cities_replaces = [\n",
    "    ['cherryhillsvillage', 'Cherry Hills Village'],\n",
    "    ['commercecity', 'Commerce City'],\n",
    "    ['federalheights', 'Federal Heights'],\n",
    "    ['bonita spgs', 'Bonita Springs'],\n",
    "    ['doctor philips', 'Orlando'],\n",
    "    ['ldhl', 'Lauderhill'],\n",
    "    ['p c beach', 'Panama City Beach'],\n",
    "    ['un-incorporated broward county', 'Fort Lauderdale'],\n",
    "    ['unincorporated broward county', 'Fort Lauderdale'],\n",
    "    ['atlaanta', 'Atlanta'],\n",
    "    ['saranac vlg', 'Saranac'],\n",
    "    ['uninc', 'Charlotte'],\n",
    "    ['west ashville', 'Ashville'],\n",
    "    ['city center', 'Las Vegas'],\n",
    "    ['bellerose manor', 'Queens Village'],\n",
    "    ['bellerose vlg', 'Bellerose Village'],\n",
    "    ['jamaica est', 'Jamaica'],\n",
    "    ['old mill basin', 'Brooklyn'],\n",
    "    ['downtown pgh', 'Pittsburgh'],\n",
    "    ['outside area (outside ca)', 'Nashville'],\n",
    "    ['unicorp/memphis', 'Memphis'],\n",
    "    ['botines', 'Laredo'],\n",
    "    ['brookside vl', 'Brookside Village'],\n",
    "    ['bville', 'Brownsville'],\n",
    "    ['clear lk shrs', 'Clear Lake Shores'],\n",
    "    ['hollywood pa', 'Hollywood Park'],\n",
    "    ['la moca', 'Laredo'],\n",
    "    ['longvi', 'Longview'],\n",
    "    ['mc allen', 'Mcallen'],\n",
    "    ['mc gregor', 'Mcgregor'],\n",
    "    ['mc kinney', 'Mckinney'],\n",
    "    ['romayor', 'Cleveland'],\n",
    "    ['s.a.', 'San Antonio'],\n",
    "    ['tarkington prairie', 'Cleveland'],\n",
    "    ['belllingham', 'Bellingham'],\n",
    "    ['china spring', np.NaN],\n",
    "    ['other city - in the state of florida', np.NaN],\n",
    "    ['other city not in the state of florida', np.NaN],\n",
    "    ['other city value - out of area', np.NaN],\n",
    "    ['other city value out of area', np.NaN],\n",
    "    ['unincorporated dade county', np.NaN],\n",
    "    ['foreign country', np.NaN],\n",
    "    ['other', np.NaN],\n",
    "    [' ', np.NaN],\n",
    "    ['--', np.NaN]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s_repl in states_replace:\n",
    "    mask = (df['state'] == s_repl[0])\n",
    "    df.loc[mask, 'state'] = s_repl[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c_repl in cities_replaces:\n",
    "    mask = (df['city'] == c_repl[0])\n",
    "    df.loc[mask, 'city'] = np.NaN if pd.isna(c_repl[1]) else str.lower(c_repl[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['city'], axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По данным спискам производится корректировка названий штатов и городов. Не найденные объекты удаляются из датасета.\n",
    "\n",
    "Производится уточняющий поиск геоданных по тсправленным городам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_list = df.groupby(['state', 'city'])['target'].mean().index\n",
    "\n",
    "for city_item in cities_list:\n",
    "    key = data_transform.get_city_dict_key(city_item[0], city_item[1])\n",
    "    if key not in cities_dict:\n",
    "        try:\n",
    "            location = geolocator.geocode(key)\n",
    "            cities_dict[key] = {\n",
    "                'type': location.raw['type'],\n",
    "                'importance': location.raw['importance'],\n",
    "                'boundingbox': location.raw['boundingbox'],\n",
    "                'lat': location.raw['lat'],\n",
    "                'lng': location.raw['lon']\n",
    "            }\n",
    "        except Exception as ex:\n",
    "            print(city_item[0] + ', {' + city_item[1] + '}', ex)\n",
    "            \n",
    "with open('../shared_libs/data/cities_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(cities_dict, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кэш-словарь с геоданными об объектам недвижимости\n",
    "Данные получены через сервисы OpenStreetMap, GoogleMap и Census\n",
    "\n",
    "Ключем словаря служит пара \\<state\\>, \\<city\\>, \\<street\\> (название города приводится к нижнему регистру)\n",
    "\n",
    "По каждому городу сохраняется следующая информация:\n",
    "* *lat* - широта объекта\n",
    "* *lng* - долгота объекта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_dict = {}\n",
    "not_founded_adresses = set([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by OSM\n",
    "\n",
    "total_count = 0\n",
    "found_count = 0\n",
    "error_count = 0\n",
    "\n",
    "max_iters_count = 250000\n",
    "\n",
    "for index, rec in df.iterrows():\n",
    "    address = data_transform.get_address_dict_key(rec['state'], rec['city'], rec['street'])\n",
    "    \n",
    "    if (address not in address_dict) and (address not in not_founded_adresses):\n",
    "        total_count += 1\n",
    "        \n",
    "        loc_rec = data_transform.get_address_location_info_by_osm(address, print_error=True)\n",
    "        \n",
    "        if loc_rec is not None:\n",
    "            address_dict[address] = loc_rec\n",
    "            found_count += 1\n",
    "            \n",
    "            if found_count % 50 == 0:\n",
    "                print('Processed:', total_count, 'Success:', found_count/total_count*100, 'Error:', error_count/total_count*100)\n",
    "                \n",
    "                with open('../shared_libs/data/address_dict.pkl', 'wb') as f:\n",
    "                    pickle.dump(address_dict, f)\n",
    "        else:\n",
    "            error_count += 1\n",
    "            not_founded_adresses.add(address)\n",
    "            \n",
    "            print('Processed:', total_count, 'Success:', found_count/total_count*100, 'Error:', error_count/total_count*100)\n",
    "            \n",
    "    if total_count >= max_iters_count:\n",
    "        break\n",
    "\n",
    "with open('../shared_libs/data/address_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(address_dict, f)\n",
    "\n",
    "print('Processed:', total_count, 'Success:', found_count/total_count*100, 'Error:', error_count/total_count*100)\n",
    "print(list(not_founded_adresses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by Google Maps API\n",
    "\n",
    "gmaps_api_key = 'GMap API key'\n",
    "\n",
    "total_count = 0\n",
    "found_count = 0\n",
    "error_count = 0\n",
    "\n",
    "max_iters_count = 250000\n",
    "\n",
    "for index, rec in df.iterrows():\n",
    "    address = data_transform.get_address_dict_key(rec['state'], rec['city'], rec['street'])\n",
    "    \n",
    "    if (address not in address_dict) and (address not in not_founded_adresses):\n",
    "        total_count += 1\n",
    "        \n",
    "        loc_rec = data_transform.get_address_location_info(address, gmaps_api_key, print_error=True)\n",
    "        \n",
    "        if loc_rec is not None:\n",
    "            address_dict[address] = loc_rec\n",
    "            found_count += 1\n",
    "            \n",
    "            if found_count % 50 == 0:\n",
    "                print('Processed:', total_count, 'Success:', found_count/total_count*100, 'Error:', error_count/total_count*100)\n",
    "                \n",
    "                with open('../shared_libs/data/address_dict.pkl', 'wb') as f:\n",
    "                    pickle.dump(address_dict, f)                \n",
    "        else:\n",
    "            error_count += 1\n",
    "            not_founded_adresses.add(address)\n",
    "            \n",
    "            print('Processed:', total_count, 'Success:', found_count/total_count*100, 'Error:', error_count/total_count*100)\n",
    "            \n",
    "    if total_count >= max_iters_count:\n",
    "        break\n",
    "\n",
    "with open('../shared_libs/data/address_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(address_dict, f)\n",
    "\n",
    "print('Processed:', total_count, 'Success:', found_count/total_count*100, 'Error:', error_count/total_count*100)\n",
    "print(list(not_founded_adresses))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кэш-словарь с геоданными по почтовому индексу\n",
    "Данные получены через сервисы OpenStreetMap и Census\n",
    "\n",
    "Для объектов по которым не были найдены геоданные (по какой либо причине) производится поиск цента района (используется почтовый индекс).\n",
    "\n",
    "Таким образом если объект присутствует в кэш-словаре адресов - то берутся координаты из этого словаря иначе берутся геоданные из словаря по почтовому индексу.\n",
    "\n",
    "Ключем словаря служит пара \\<state\\>, \\<city\\>, \\<zipcode\\> (название города приводится к нижнему регистру)\n",
    "\n",
    "По каждому городу сохраняется следующая информация:\n",
    "* *lat* - широта объекта\n",
    "* *lng* - долгота объекта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_by_zip_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by OSM (zipcode)\n",
    "\n",
    "total_count = 0\n",
    "found_count = 0\n",
    "error_count = 0\n",
    "\n",
    "max_iters_count = 250000\n",
    "\n",
    "for index, rec in df.iterrows():\n",
    "    address = data_transform.get_address_dict_key(rec['state'], rec['city'], rec['street'])\n",
    "    address_zip = data_transform.get_address_zip_dict_key(rec['state'], rec['city'], rec['zipcode'])\n",
    "    \n",
    "    if (address not in address_dict) and (address_zip not in address_by_zip_dict):\n",
    "        total_count += 1\n",
    "        \n",
    "        loc_rec = data_transform.get_address_location_info_by_osm(rec['state']+', '+rec['city']+', '+rec['zipcode'], print_error=True)\n",
    "        \n",
    "        if loc_rec is not None:\n",
    "            address_by_zip_dict[address_zip] = loc_rec\n",
    "            found_count += 1\n",
    "            \n",
    "            if found_count % 50 == 0:\n",
    "                print('Processed:', total_count, 'Success:', found_count/total_count*100, 'Error:', error_count/total_count*100)\n",
    "                \n",
    "                with open('../shared_libs/data/address_by_zip_dict.pkl', 'wb') as f:\n",
    "                    pickle.dump(address_by_zip_dict, f)                \n",
    "        else:\n",
    "            error_count += 1\n",
    "            not_founded_adresses.add(address)\n",
    "            \n",
    "            print('Processed:', total_count, 'Success:', found_count/total_count*100, 'Error:', error_count/total_count*100)\n",
    "            \n",
    "    if total_count >= max_iters_count:\n",
    "        break\n",
    "\n",
    "with open('../shared_libs/data/address_by_zip_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(address_by_zip_dict, f)\n",
    "\n",
    "print('Processed:', total_count, 'Success:', found_count/total_count*100, 'Error:', error_count/total_count*100)\n",
    "print(list(not_founded_adresses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_zipcodes_index = df.groupby(['state', 'city', 'zipcode']).agg({'street': 'count'}).sort_values(by='street', ascending=False).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by US Census (zipcode)\n",
    "\n",
    "total_count = 0\n",
    "found_count = 0\n",
    "error_count = 0\n",
    "\n",
    "for index in all_zipcodes_index:\n",
    "    address_zip = data_transform.get_address_zip_dict_key(index[0], index[1], index[2])\n",
    "    \n",
    "    if address_zip not in address_by_zip_dict:\n",
    "        total_count += 1\n",
    "        \n",
    "        loc_rec = data_transform.get_zipcode_location_info_by_us_census(index[0], index[1], index[2], print_error=True)\n",
    "        \n",
    "        if loc_rec is not None:\n",
    "            address_by_zip_dict[address_zip] = loc_rec\n",
    "            found_count += 1\n",
    "            \n",
    "            if found_count % 50 == 0:\n",
    "                print('Processed:', total_count, 'Success:', found_count/total_count*100, 'Error:', error_count/total_count*100)\n",
    "                \n",
    "                with open('../shared_libs/data/address_by_zip_dict.pkl', 'wb') as f:\n",
    "                    pickle.dump(address_by_zip_dict, f)                \n",
    "        else:\n",
    "            error_count += 1\n",
    "            not_founded_adresses.add(address_zip)\n",
    "            \n",
    "            print('Processed:', total_count, 'Success:', found_count/total_count*100, 'Error:', error_count/total_count*100)\n",
    "            \n",
    "with open('../shared_libs/data/address_by_zip_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(address_by_zip_dict, f)\n",
    "\n",
    "print('Processed:', total_count, 'Success:', found_count/total_count*100, 'Error:', error_count/total_count*100)\n",
    "print(list(not_founded_adresses))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Формирование словаря кластеров недвижимости по цене объектов\n",
    "\n",
    "Для каждого города формируется два кластера (наболее дорогоая недвижимость и ниболее дешевая).\n",
    "\n",
    "По каждому городу берется верхний и нижний перцентиль (20%) и для этих групп берется медиана широты и долготы. Эти координаты и принимаются за центр кластера, относительно которого и вычисляется нормированное расстояние до объекта недвижимости."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_clusters_dict = {}\n",
    "all_cities = df.groupby(['state', 'city']).agg({'city': 'count'}).rename({'city': 'city_count'}, axis=1).sort_values(by=['city_count'], ascending=False).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERCENTILE_SIZE = 20\n",
    "\n",
    "for city_indx in all_cities:\n",
    "    state = city_indx[0]\n",
    "    city = city_indx[1]\n",
    "    \n",
    "    city_key = data_transform.get_city_dict_key(state, city)\n",
    "    cities_clusters_dict[city_key] = data_transform.get_subset_mean_location(df, state, city, PERCENTILE_SIZE, cities_dict, address_dict, address_by_zip_dict)\n",
    "\n",
    "with open('../shared_libs/data/cities_clusters_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(cities_clusters_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
